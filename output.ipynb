{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eebc8c1-02a6-42a8-9db5-eb964f0f8ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "GITHUB_URL = \"https://github.com/ashhhwin/Multi-Agent-Customer-Service-System-with-A2A-and-MCP---Google-ADK.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b2950a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY found\n",
      "GEMINI_API_KEY: AIzaSyALlHCE3F4IGMKg1MEREHclWST7zVBj3ao\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "\n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"ERROR: GEMINI_API_KEY not found in environment variables!\")\n",
    "    print(\"Please create a .env file with your API key.\")\n",
    "else:\n",
    "    print(\"GEMINI_API_KEY found\")\n",
    "    print(f\"GEMINI_API_KEY: {GEMINI_API_KEY}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "132cb014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
    ")\n",
    "\n",
    "logging.getLogger(\"uvicorn.access\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)\n",
    "\n",
    "print(\"Logging configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0bfa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "import a2a_patch\n",
    "import database_utility\n",
    "import service_tools\n",
    "import agent_definitions\n",
    "\n",
    "from server_launcher import start_server_daemon\n",
    "from client_runner import execute_test_suite\n",
    "\n",
    "print(\"All modules imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ebf5874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING MULTI-AGENT SYSTEM\n",
      "================================================================================\n",
      "\n",
      "Waiting for all services to initialize...\n",
      "\n",
      "============================================================\n",
      "Initiating All Microservices...\n",
      "============================================================\n",
      "Initializing MCP Data Server...\n",
      "Database schema initialized and seeded at service_db.sqlite\n",
      "\n",
      "[MCP READY] Data Access Server listening on http://127.0.0.1:8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [28079]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Customer Info Agent starting on http://127.0.0.1:9300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [28079]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Support Specialist Agent starting on http://127.0.0.1:9301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [28079]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9300 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9301 (Press CTRL+C to quit)\n",
      "INFO:     Uvicorn running on http://127.0.0.1:9400 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Orchestration Agent starting on http://127.0.0.1:9400\n",
      "\n",
      "[READY] All service servers deployed!\n",
      "    - Orchestration Entry Point: http://127.0.0.1:9400\n",
      "============================================================\n",
      "\n",
      "\n",
      "All servers running!\n",
      "\n",
      "Service Endpoints:\n",
      "   MCP Server:              http://127.0.0.1:8000\n",
      "   Customer Info Agent:     http://127.0.0.1:9300\n",
      "   Support Specialist:      http://127.0.0.1:9301\n",
      "   Orchestration Agent:     http://127.0.0.1:9400\n",
      "================================================================================\n",
      "INFO:     127.0.0.1:65382 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65384 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:45:56,143 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: fb7b0cec-2d3c-4d51-b4b6-a1c668896cab, context_id: c4326e5e-5fac-4800-8e1f-cc727dbddfc6).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65386 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:45:56,147 - INFO - a2a.client.card_resolver - Successfully fetched agent card data from http://localhost:9300/.well-known/agent-card.json: {'capabilities': {'streaming': True}, 'defaultInputModes': ['text/plain'], 'defaultOutputModes': ['text/plain', 'application/json'], 'description': 'Specialized system for secure access and management of customer records and data via a service layer.', 'name': 'Customer Information System', 'preferredTransport': 'JSONRPC', 'protocolVersion': '0.3.0', 'skills': [{'description': 'Fetches account details using the unique customer identifier.', 'examples': ['Find the record for ID 1', 'Retrieve customer 5 information'], 'id': 'get_details', 'name': 'Retrieve Customer Details', 'tags': ['customer', 'data', 'lookup']}, {'description': 'Amends customer fields such as email or phone.', 'examples': ['Update email for account 1', 'Change phone number for customer 5'], 'id': 'update_record', 'name': 'Modify Customer Record', 'tags': ['customer', 'update', 'modify']}, {'description': 'Execute complex queries requiring multiple database operations and filtering.', 'examples': ['Find all active accounts with open tickets', 'List customers with high priority issues'], 'id': 'complex_queries', 'name': 'Multi-Step Data Operations', 'tags': ['customer', 'search', 'filter', 'analysis']}], 'url': 'http://localhost:9300', 'version': '1.0'}\n",
      "2025-12-03 13:45:56,147 - INFO - google_adk.google.adk.agents.remote_a2a_agent - Successfully resolved remote A2A agent: information_system\n",
      "\u001b[92m13:45:56 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:45:56,158 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 7231045b-cd2d-41e6-ba90-0343204c3576, context_id: b32ad2a9-0cca-40cb-805f-ce51e015aad4).\n",
      "2025-12-03 13:45:56,158 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65390 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:45:57 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:45:57,248 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65386 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65392 - \"GET /.well-known/agent-card.json HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:45:58,110 - INFO - a2a.client.card_resolver - Successfully fetched agent card data from http://localhost:9301/.well-known/agent-card.json: {'capabilities': {'streaming': True}, 'defaultInputModes': ['text/plain'], 'defaultOutputModes': ['text/plain'], 'description': 'Dedicated agent for handling service inquiries, issue logging, and resolution.', 'name': 'Support Specialist', 'preferredTransport': 'JSONRPC', 'protocolVersion': '0.3.0', 'skills': [{'description': 'Logs a new ticket with customer ID, issue description, and priority level.', 'examples': ['Log a ticket for customer 1 about account upgrade', 'Create high priority billing issue'], 'id': 'log_issue', 'name': 'Register New Support Ticket', 'tags': ['support', 'ticket', 'create']}, {'description': 'Processes standard support questions and delivers a resolution or advice.', 'examples': ['I need help with my account', 'How do I upgrade my subscription?'], 'id': 'resolve_query', 'name': 'Address Customer Inquiry', 'tags': ['support', 'help', 'assistance']}], 'url': 'http://localhost:9301', 'version': '1.0'}\n",
      "2025-12-03 13:45:58,110 - INFO - google_adk.google.adk.agents.remote_a2a_agent - Successfully resolved remote A2A agent: specialist_support\n",
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:45:58,116 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 39adb5fa-1995-47f0-aec3-5cf452338888, context_id: 1cddce76-3a5c-4b63-95fb-ed74566c6a31).\n",
      "2025-12-03 13:45:58,115 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65393 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:45:58 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:45:58,812 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65392 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65395 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:46:04,919 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 7bc3ba35-ff1c-4b7e-9e18-e24627a48820, context_id: 5093f579-fd1c-4513-a047-37904705f06a).\n",
      "\u001b[92m13:46:04 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:04,926 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 21628def-6d85-4f88-9a73-8b0deb5b61ff, context_id: 7a579433-acb8-4ce5-b873-b49cbc24f5e6).\n",
      "2025-12-03 13:46:04,926 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "\u001b[92m13:46:05 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:05,488 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65397 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:06 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:06,395 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: b1c4b4eb-5907-4aec-a9e1-1dffa0a785cd, context_id: 5cb41182-27d5-4bf1-afe3-a1d26e162eed).\n",
      "2025-12-03 13:46:06,394 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65402 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:07 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:07,238 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65401 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65405 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:46:12,928 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 1f3b39a7-28ef-42e2-a9c8-2f3bab4719ac, context_id: cd1507b3-48ca-4f64-9b18-641a782af894).\n",
      "\u001b[92m13:46:12 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:12,933 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: d1d6d3b6-2678-43b7-ba44-d48f84ad2ebc, context_id: 6cd32795-45b9-4a74-bb16-3be78c69b74e).\n",
      "2025-12-03 13:46:12,932 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65409 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:13 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:13,673 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65407 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:15 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:15,298 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 4cf835eb-a106-4ebb-ad80-4b264ded5143, context_id: 715cff4e-def1-4a90-813a-2911b6861a17).\n",
      "2025-12-03 13:46:15,297 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65417 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65418 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65419 - \"POST /call HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65420 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:16 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:16,652 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65413 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65425 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:46:23,103 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 2733914a-fb85-4948-9e84-afa964578370, context_id: 3cb89dd3-8ef4-4c3f-85ca-0eeb9ef4f8fd).\n",
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:23,111 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: b529ee3b-502f-4816-86d6-218d1a66dfb2, context_id: 0c26ae3e-a2ea-4062-b252-20ab37d5ef50).\n",
      "2025-12-03 13:46:23,111 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65433 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:23 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:23,955 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65427 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:25 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:25,349 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 5e5ff794-c543-4ba6-bc75-088f1d8be99e, context_id: 4a1b99b1-7230-4f82-ad91-551c9cdd51ed).\n",
      "2025-12-03 13:46:25,349 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65436 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:26 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:26,295 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65435 - \"POST / HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:65438 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:46:32,287 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: fbaf0108-86bb-46e6-94d7-343ec3b89e09, context_id: 1bc29832-8ec0-4557-9b90-fa0b7be23f92).\n",
      "\u001b[92m13:46:32 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:32,293 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: 02ce4952-e170-4656-ac3a-ae122752c826, context_id: 66e2e4ae-9df8-41d6-86f2-03db96b1efe4).\n",
      "2025-12-03 13:46:32,293 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65442 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:33 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:33,075 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65440 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:34 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:34,074 - INFO - a2a.server.tasks.task_manager - Task not found or task_id not set. Creating new task for event (task_id: f64f8881-a64e-4c3e-a63b-7258f4868339, context_id: b3afb974-bcd7-4c03-bb5d-5ddf91cf3cc5).\n",
      "2025-12-03 13:46:34,074 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65445 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:34 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:34,954 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65446 - \"POST /call HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m13:46:35 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n",
      "2025-12-03 13:46:35,695 - INFO - LiteLLM - \n",
      "LiteLLM completion() model= together/meta-llama/Llama-3.2-3B-Instruct; provider = huggingface\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:65444 - \"POST / HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING MULTI-AGENT SYSTEM\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "server_thread = start_server_daemon()\n",
    "\n",
    "print(\"\\nWaiting for all services to initialize...\")\n",
    "time.sleep(8)\n",
    "\n",
    "print(\"\\nAll servers running!\")\n",
    "print(\"\\nService Endpoints:\")\n",
    "print(\"   MCP Server:              http://127.0.0.1:8000\")\n",
    "print(\"   Customer Info Agent:     http://127.0.0.1:9300\")\n",
    "print(\"   Support Specialist:      http://127.0.0.1:9301\")\n",
    "print(\"   Orchestration Agent:     http://127.0.0.1:9400\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c144bf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RUNNING INTEGRATION TEST SUITE\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "INTEGRATION TEST SUITE - Multi-Agent Service System\n",
      "================================================================================\n",
      "\n",
      "[1/5] CASE 1: Simple Data Retrieval\n",
      "Notes: Routes to Information Agent for a basic lookup.\n",
      "Expected: Should return full customer record with all fields\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ RESPONSE:\n",
      "Based on the customer's information, I will create a support ticket for them.\n",
      "\n",
      "The customer requested a password reset assistance. I will create a ticket with the following details:\n",
      "\n",
      "- Customer ID: 1\n",
      "- Query Description: \"Password reset assistance\"\n",
      "- Urgency Level: low\n",
      "\n",
      "Please confirm the ticket creation with the ticket ID. \n",
      "\n",
      "I will now create the ticket using the register_support_issue tool.\n",
      "\n",
      "[2/5] CASE 2: Coordinated Service Request\n",
      "Notes: Router should delegate to Support Agent to create an upgrade ticket.\n",
      "Expected: Should create a support ticket for service upgrade request\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ RESPONSE:\n",
      "The support ticket has been created successfully. The ticket ID is 8. Please proceed with updating your customer record using the provided JSON string.\n",
      "\n",
      "[3/5] CASE 3: Complex Filtering Query\n",
      "Notes: Requires multiple tool calls: get active accounts, check each for tickets, filter.\n",
      "Expected: Should list active accounts with at least one open ticket\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ RESPONSE:\n",
      "Here are the active customer accounts with open support tickets:\n",
      "\n",
      "1. Alice Premium (identifier: 1) - contact_email: alice@example.com, contact_phone: 111-111-1111\n",
      "2. Bob Standard (identifier: 2) - contact_email: bob@example.com, contact_phone: 222-222-2222\n",
      "3. Diana Premium (identifier: 4) - contact_email: diana@example.com, contact_phone: 444-444-4444\n",
      "4. Eve Standard (identifier: 5) - contact_email: eve@example.com, contact_phone: 555-555-5555\n",
      "5. Priya Patel (Premium) (identifier: 12345) - contact_email: priya@example.com, contact_phone: 555-0999\n",
      "\n",
      "[4/5] CASE 4: High-Priority Ticket Logging\n",
      "Notes: Support Agent must recognize urgency and log a 'high' priority ticket.\n",
      "Expected: Should create high priority ticket for billing/refund issue\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ RESPONSE:\n",
      "I have created a new support ticket with ticket ID 10, account ID 1, description \"Refund for duplicate subscription charge\", status \"open\", priority level \"high\", and submission timestamp December 3, 2025, 7:46:26 PM.\n",
      "\n",
      "Please let me know if there's any further assistance I can provide.\n",
      "\n",
      "[5/5] CASE 5: Multi-Step Record Update and History Check\n",
      "Notes: Sequential: Update email (Data Agent) then retrieve history (Data Agent).\n",
      "Expected: Should update email successfully and display ticket history\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "✓ RESPONSE:\n",
      "I have updated the customer's email to newaddress@corp.com. Here is the complete ticket history for customer ID 5:\n",
      "\n",
      "- Ticket ID 5:\n",
      "  - Account ID: 5\n",
      "  - Description: Password reset\n",
      "  - Status: open\n",
      "  - Priority level: low\n",
      "  - Submission timestamp: 2025-12-03T19:45:34.661237+00:00\n",
      "\n",
      "================================================================================\n",
      "TEST SUITE SUMMARY\n",
      "================================================================================\n",
      "✓ PASS - CASE 1: Simple Data Retrieval\n",
      "✓ PASS - CASE 2: Coordinated Service Request\n",
      "✓ PASS - CASE 3: Complex Filtering Query\n",
      "✓ PASS - CASE 4: High-Priority Ticket Logging\n",
      "✓ PASS - CASE 5: Multi-Step Record Update and History Check\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Test suite completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RUNNING INTEGRATION TEST SUITE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    await execute_test_suite()\n",
    "    print(\"\\nTest suite completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nTest suite encountered an error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
